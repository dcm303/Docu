
# Gi·∫£i M√£ Backpropagation: V√≠ d·ª• S·ªë li·ªáu C·ª• th·ªÉ & C√¥ng th·ª©c

ƒê·ªÉ hi·ªÉu t∆∞·ªùng t·∫≠n "ma thu·∫≠t" c·ªßa Backpropagation, ch√∫ng ta c·∫ßn ƒëi s√¢u v√†o s·ªë li·ªáu v√† c√¥ng th·ª©c to√°n h·ªçc. D∆∞·ªõi ƒë√¢y l√† m√¥ ph·ªèng quy tr√¨nh tr√™n m·ªôt **M√¥ h√¨nh CNN t√≠ hon**.

---

## üêØ K·ªãch b·∫£n v√≠ d·ª•: "Ph√°t hi·ªán s·ªçc h·ªï"

**M·ª•c ti√™u:** Hu·∫•n luy·ªán m·∫°ng nh·∫≠n di·ªán xem ·∫£nh c√≥ "S·ªçc d·ªçc" hay kh√¥ng.

* **Input ($X$):** M·ªôt m·∫£nh ·∫£nh b√© x√≠u 3x1 pixel. Gi√° tr·ªã `[10, 10, 0]` (S√°ng - S√°ng - T·ªëi $\to$ C√≥ bi√™n d·ªçc).
* **Nh√£n th·∫≠t ($Y_{true}$):** $1$ (L√† H·ªï).
* **Ki·∫øn tr√∫c:** 1 L·ªõp Conv (1 Filter 2x1) $\to$ 1 L·ªõp Output (T·ªïng Feature Map).

---

### PH·∫¶N 1: FORWARD PASS (T√çNH TO√ÅN XU√îI)
*M·ª•c ti√™u: Xem m·∫°ng hi·ªán t·∫°i (ƒëang "ngu ng∆°") ƒëo√°n ra c√°i g√¨.*

**B∆∞·ªõc 1: Convolution**
M√°y t√≠nh kh·ªüi t·∫°o ng·∫´u nhi√™n m·ªôt Filter ($W_1$) k√≠ch th∆∞·ªõc 2x1. Gi·∫£ s·ª≠ kh·ªüi t·∫°o: `[0.5, -0.5]`.
Ta tr∆∞·ª£t Filter n√†y qua Input `[10, 10, 0]`:

* **V·ªã tr√≠ 1 (Pixel 1, 2):** `[10, 10]` nh√¢n v·ªõi `[0.5, -0.5]`
    $$(10 \times 0.5) + (10 \times -0.5) = 5 - 5 = 0$$
* **V·ªã tr√≠ 2 (Pixel 2, 3):** `[10, 0]` nh√¢n v·ªõi `[0.5, -0.5]`
    $$(10 \times 0.5) + (0 \times -0.5) = 5 - 0 = 5$$

$\Rightarrow$ **Feature Map:** `[0, 5]`

**B∆∞·ªõc 2: Ra quy·∫øt ƒë·ªãnh (Output Layer)**
C·ªông t·ªïng Feature Map ƒë·ªÉ ra ƒëi·ªÉm s·ªë cu·ªëi c√πng.

$$Y_{pred} = 0 + 5 = 5$$

---

### PH·∫¶N 2: T√çNH TO√ÅN SAI S·ªê (LOSS)
*M·ª•c ti√™u: Bi·∫øt m·∫°ng ƒëang sai bao nhi√™u.*

Ta mong mu·ªën k·∫øt qu·∫£ l√† **10** (V√≠ d·ª• quy ƒë·ªãnh), nh∆∞ng m√°y ch·ªâ ra **5**.
C√¥ng th·ª©c Loss (B√¨nh ph∆∞∆°ng sai s·ªë):

$$L = (Y_{pred} - Y_{true})^2$$

T√≠nh to√°n:
$$L = (5 - 10)^2 = (-5)^2 = 25$$

$\Rightarrow$ **K·∫øt lu·∫≠n:** L·ªói l√† 25. R·∫•t l·ªõn! C·∫ßn s·ª≠a Filter `[0.5, -0.5]` ngay.

---

### PH·∫¶N 3: BACKPROPAGATION (TRUY T√åM TR√ÅCH NHI·ªÜM)
*M·ª•c ti√™u: Bi·∫øt c·∫ßn s·ª≠a s·ªë 0.5 hay -0.5, v√† s·ª≠a bao nhi√™u.*

Ta c·∫ßn t√≠nh ƒê·∫°o h√†m c·ªßa L·ªói theo Tr·ªçng s·ªë ($W$): $\frac{\partial L}{\partial W}$.
√Åp d·ª•ng **Quy t·∫Øc chu·ªói (Chain Rule)**:

$$\frac{\partial L}{\partial W} = \underbrace{\frac{\partial L}{\partial Y_{pred}}}_{\text{L·ªói do D·ª± ƒëo√°n}} \times \underbrace{\frac{\partial Y_{pred}}{\partial W}}_{\text{D·ª± ƒëo√°n do Tr·ªçng s·ªë}}$$

**Kh√∫c 1: L·ªói thay ƒë·ªïi th·∫ø n√†o theo D·ª± ƒëo√°n?**
H√†m Loss: $L = (Y - 10)^2$. ƒê·∫°o h√†m l√† $2 \times (Y - 10)$.
$$\frac{\partial L}{\partial Y_{pred}} = 2 \times (5 - 10) = -10$$
*(√ù nghƒ©a: S·ªë √¢m nghƒ©a l√† D·ª± ƒëo√°n ƒëang th·∫•p qu√°, c·∫ßn tƒÉng l√™n).*

**Kh√∫c 2: D·ª± ƒëo√°n thay ƒë·ªïi th·∫ø n√†o theo Tr·ªçng s·ªë?**

Nh√¨n l·∫°i c√¥ng th·ª©c Forward t·∫°i v·ªã tr√≠ t·∫°o ra k·∫øt qu·∫£ 5:
$$\text{K·∫øt qu·∫£} = (\text{Pixel}_2 \times W_{\text{tr√°i}}) + (\text{Pixel}_3 \times W_{\text{ph·∫£i}})$$
$$5 = (10 \times 0.5) + (0 \times -0.5)$$

* V·ªõi $W_{\text{tr√°i}}$ (0.5): Nh√¢n v·ªõi Pixel 2 (gi√° tr·ªã 10) $\to$ ƒê·∫°o h√†m l√† **10**.
* V·ªõi $W_{\text{ph·∫£i}}$ (-0.5): Nh√¢n v·ªõi Pixel 3 (gi√° tr·ªã 0) $\to$ ƒê·∫°o h√†m l√† **0**.

**T·ªîNG H·ª¢P (Gradient):**

1.  **Gradient cho $W_{\text{tr√°i}}$:**
    $$\text{Grad}_1 = (-10) \times 10 = -100$$
    *(C·∫ßn tƒÉng tr·ªçng s·ªë n√†y th·∫≠t m·∫°nh).*

2.  **Gradient cho $W_{\text{ph·∫£i}}$:**
    $$\text{Grad}_2 = (-10) \times 0 = 0$$
    *(Kh√¥ng ƒë√≥ng g√≥p v√†o l·ªói sai, kh√¥ng c·∫ßn s·ª≠a).*

---

### PH·∫¶N 4: WEIGHT UPDATE (S·ª¨A SAI)
*M·ª•c ti√™u: Ra ƒë∆∞·ª£c Filter m·ªõi x·ªãn h∆°n.*

C√¥ng th·ª©c c·∫≠p nh·∫≠t (v·ªõi Learning Rate $\eta = 0.01$):
$$W_{\text{m·ªõi}} = W_{\text{c≈©}} - (\eta \times \text{Gradient})$$

* **S·ª≠a $W_{\text{tr√°i}}$ (C≈© l√† 0.5):**
    $$W_{\text{m·ªõi}} = 0.5 - (0.01 \times -100) = 0.5 - (-1) = 1.5$$
* **S·ª≠a $W_{\text{ph·∫£i}}$ (C≈© l√† -0.5):**
    $$W_{\text{m·ªõi}} = -0.5 - (0.01 \times 0) = -0.5$$

$\Rightarrow$ **Filter M·ªõi:** `[1.5, -0.5]`.

---

### T·ªîNG K·∫æT
C√¥ng th·ª©c c·ªët l√µi ƒë·ªÉ m·∫°ng Deep Learning t√¨m ra tham s·ªë t·ªëi ∆∞u:

$$W_{new} = W_{old} - \eta \cdot \nabla Loss$$

Logic: N·∫øu Input $X$ l·ªõn m√† g√¢y ra l·ªói, th√¨ $W$ ph·∫£i ch·ªãu tr√°ch nhi·ªám l·ªõn. N·∫øu Input $X=0$, th√¨ $W$ v√¥ t·ªôi.